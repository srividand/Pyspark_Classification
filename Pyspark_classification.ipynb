{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case description\n",
    "\n",
    "A supermarket is offering a new line of organic products. The supermarket's management wants to determine which customers are likely to purchase these products.\n",
    "The supermarket has a customer loyalty program. As an initial buyer incentive plan, the supermarket provided coupons for the organic products to all of the loyalty program participants and collected data that includes whether these customers purchased any of the organic products.\n",
    "The ORGANICS data set contains 13 variables and 22222 observations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+------+----------+---------------+---------+----------+------------+---------+---------+--------+---------+---------+\n",
      "|   ID|DemAffl|DemAge|DemCluster|DemClusterGroup|DemGender|    DemReg|    DemTVReg|PromClass|PromSpend|PromTime|TargetBuy|TargetAmt|\n",
      "+-----+-------+------+----------+---------------+---------+----------+------------+---------+---------+--------+---------+---------+\n",
      "|  140|     10|    76|        16|              C|        U|  Midlands|Wales & West|     Gold|  16000.0|       4|        0|        0|\n",
      "|  620|      4|    49|        35|              D|        U|  Midlands|Wales & West|     Gold|   6000.0|       5|        0|        0|\n",
      "|  868|      5|    70|        27|              D|        F|  Midlands|Wales & West|   Silver|     0.02|       8|        1|        1|\n",
      "| 1120|     10|    65|        51|              F|        M|  Midlands|    Midlands|      Tin|     0.01|       7|        1|        1|\n",
      "| 2313|     11|    68|         4|              A|        F|  Midlands|    Midlands|      Tin|     0.01|       8|        0|        0|\n",
      "| 2771|      9|    72|        28|              D|        U|     North|      N West| Platinum| 20759.81|       3|        0|        0|\n",
      "| 3131|     11|    74|         3|              A|        F|  Midlands|        East|      Tin|     0.01|       8|        0|        0|\n",
      "| 3328|     13|    62|        32|              D|        M|     North|      N East|      Tin|     0.01|       5|        0|        0|\n",
      "| 4529|     10|    62|        49|              F|        M|  Midlands|        East|   Silver|  2038.76|       3|        0|        0|\n",
      "| 5886|     14|    43|        49|              F|        F|      null|        null|     Gold|   6000.0|       1|        1|        1|\n",
      "| 7420|      7|    60|        52|              F|        F|     North|      N East|     Gold|  11000.0|       2|        0|        0|\n",
      "| 9814|      5|  null|        24|              C|        M|South East|      London|   Silver|   5000.0|       1|        1|        1|\n",
      "|10006|      9|    51|        52|              F|        F|  Midlands|    Midlands|   Silver|    300.0|      11|        0|        0|\n",
      "|10219|      6|    64|        19|              C|        F|South East|  S & S East|      Tin|     0.01|       9|        0|        0|\n",
      "|10812|     16|    37|        18|              C|        F|South East|      London|      Tin|     0.01|       4|        1|        2|\n",
      "|11207|      8|    54|        28|              D|        M|  Midlands|    Midlands|   Silver|   1420.0|       1|        0|        0|\n",
      "|11932|      5|    70|        15|              B|        F|  Midlands|    Midlands|     Gold|  6104.66|       8|        0|        0|\n",
      "|14656|   null|    42|        19|              C|        F|  Midlands|        East|      Tin|     0.01|       5|        1|        1|\n",
      "|15350|      7|  null|        40|              E|        F|  Scottish|  C Scotland|      Tin|     0.01|       5|        1|        1|\n",
      "|17302|      7|    49|        28|              D|     null|South East|      London|      Tin|     0.01|       7|        0|        0|\n",
      "+-----+-------+------+----------+---------------+---------+----------+------------+---------+---------+--------+---------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "data = spark.read.csv(r\"Filepath\", inferSchema=True, header=True) # read csv data, set inferSchema to be true.\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The ID column is not needed for machine learning. The variable DemCluster is a nominal variable that includes too many categories, so, I removed this variable and also removed the dependent variable \"TargetAmt\" which is not used. I also changed the datatype of the interval independent variables including 'DemAffl', 'DemAge', and 'PromTime' from interger into double for convenient data processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DemAffl: integer (nullable = true)\n",
      " |-- DemAge: integer (nullable = true)\n",
      " |-- DemClusterGroup: string (nullable = true)\n",
      " |-- DemGender: string (nullable = true)\n",
      " |-- DemReg: string (nullable = true)\n",
      " |-- DemTVReg: string (nullable = true)\n",
      " |-- PromClass: string (nullable = true)\n",
      " |-- PromSpend: double (nullable = true)\n",
      " |-- PromTime: integer (nullable = true)\n",
      " |-- TargetBuy: integer (nullable = true)\n",
      "\n",
      "root\n",
      " |-- DemAffl: double (nullable = true)\n",
      " |-- DemAge: double (nullable = true)\n",
      " |-- DemClusterGroup: string (nullable = true)\n",
      " |-- DemGender: string (nullable = true)\n",
      " |-- DemReg: string (nullable = true)\n",
      " |-- DemTVReg: string (nullable = true)\n",
      " |-- PromClass: string (nullable = true)\n",
      " |-- PromSpend: double (nullable = true)\n",
      " |-- PromTime: double (nullable = true)\n",
      " |-- TargetBuy: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# step 1. dropping two columns ID, DemCluster, and TargetAmt\n",
    "data = data.drop(\"ID\", \"DemCluster\", \"TargetAmt\")\n",
    "data.printSchema()\n",
    "data.head(3) # this is an action. Head() returns a list of rows.\n",
    "# step 2. changing the data types of  'DemAffl', 'DemAge', and 'PromTime' to double.\n",
    "from pyspark.sql.types import DoubleType\n",
    "for variable in ['DemAffl', 'DemAge','PromTime']:\n",
    "    data = data.withColumn(variable,data[variable].cast(\"double\"))\n",
    "# printing schema again to verify if the data type conversion works. \n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2. Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------+-----------------+-----------------+\n",
      "|summary|          DemAffl|            DemAge|        PromSpend|         PromTime|\n",
      "+-------+-----------------+------------------+-----------------+-----------------+\n",
      "|  count|            21137|             20713|            22222|            21941|\n",
      "|   mean|8.711832331929791|53.796890841500506|4420.248964089997|6.564605077252632|\n",
      "| stddev|3.421194092206597|13.205780845120188|7559.046597013129|4.657208722596065|\n",
      "|    min|              0.0|              18.0|             0.01|              0.0|\n",
      "|    max|             34.0|              79.0|        296313.85|             39.0|\n",
      "+-------+-----------------+------------------+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "interval_variables = ['DemAffl','DemAge', 'PromSpend','PromTime']\n",
    "data.select('DemAffl','DemAge', 'PromSpend','PromTime').describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interval_with_missing = ['DemAffl','DemAge', 'PromSpend','PromTime']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "|DemClusterGroup|count|\n",
      "+---------------+-----+\n",
      "|              F| 3949|\n",
      "|           null|  674|\n",
      "|              E| 2607|\n",
      "|              B| 4144|\n",
      "|              U|   54|\n",
      "|              D| 4378|\n",
      "|              C| 4566|\n",
      "|              A| 1850|\n",
      "+---------------+-----+\n",
      "\n",
      "+---------+-----+\n",
      "|DemGender|count|\n",
      "+---------+-----+\n",
      "|        F|12148|\n",
      "|     null| 2513|\n",
      "|        M| 5815|\n",
      "|        U| 1746|\n",
      "+---------+-----+\n",
      "\n",
      "+----------+-----+\n",
      "|    DemReg|count|\n",
      "+----------+-----+\n",
      "|  Scottish| 1368|\n",
      "|      null|  466|\n",
      "|South East| 8634|\n",
      "|South West|  691|\n",
      "|  Midlands| 6740|\n",
      "|     North| 4323|\n",
      "+----------+-----+\n",
      "\n",
      "+------------+-----+\n",
      "|    DemTVReg|count|\n",
      "+------------+-----+\n",
      "|      Ulster|  266|\n",
      "|      N East|  785|\n",
      "|      Border|  203|\n",
      "|      S West|  691|\n",
      "|        null|  465|\n",
      "|      London| 6189|\n",
      "|   Yorkshire| 1443|\n",
      "|        East| 1649|\n",
      "|      N Scot|  329|\n",
      "|      N West| 2096|\n",
      "|  C Scotland|  836|\n",
      "|    Midlands| 3122|\n",
      "|Wales & West| 1703|\n",
      "|  S & S East| 2445|\n",
      "+------------+-----+\n",
      "\n",
      "+---------+-----+\n",
      "|PromClass|count|\n",
      "+---------+-----+\n",
      "|      Tin| 6487|\n",
      "| Platinum|  840|\n",
      "|   Silver| 8572|\n",
      "|     Gold| 6323|\n",
      "+---------+-----+\n",
      "\n",
      "+---------+-----+\n",
      "|TargetBuy|count|\n",
      "+---------+-----+\n",
      "|        1| 5504|\n",
      "|        0|16718|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nominal_with_dependent = ['DemClusterGroup', 'DemGender', 'DemReg', 'DemTVReg', 'PromClass', 'TargetBuy']\n",
    "for column in nominal_with_dependent:\n",
    "    data.groupBy(column).count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nominal_with_missing = ['DemClusterGroup', 'DemGender', 'DemReg', 'DemTVReg', 'PromClass', 'TargetBuy']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3. Feature Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Missing value imputation for interval variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "|DemAfflMissing|count|\n",
      "+--------------+-----+\n",
      "|             1| 1085|\n",
      "|             0|21137|\n",
      "+--------------+-----+\n",
      "\n",
      "+-------+------------------+\n",
      "|summary|    DemAfflImputed|\n",
      "+-------+------------------+\n",
      "|  count|             22222|\n",
      "|   mean| 8.711832331929804|\n",
      "| stddev|3.3366243422702384|\n",
      "|    min|               0.0|\n",
      "|    max|              34.0|\n",
      "+-------+------------------+\n",
      "\n",
      "+-------------+-----+\n",
      "|DemAgeMissing|count|\n",
      "+-------------+-----+\n",
      "|            1| 1509|\n",
      "|            0|20713|\n",
      "+-------------+-----+\n",
      "\n",
      "+-------+-----------------+\n",
      "|summary|    DemAgeImputed|\n",
      "+-------+-----------------+\n",
      "|  count|            22222|\n",
      "|   mean|53.79689084149959|\n",
      "| stddev|12.74950444653274|\n",
      "|    min|             18.0|\n",
      "|    max|             79.0|\n",
      "+-------+-----------------+\n",
      "\n",
      "+----------------+-----+\n",
      "|PromSpendMissing|count|\n",
      "+----------------+-----+\n",
      "|               0|22222|\n",
      "+----------------+-----+\n",
      "\n",
      "+-------+-----------------+\n",
      "|summary| PromSpendImputed|\n",
      "+-------+-----------------+\n",
      "|  count|            22222|\n",
      "|   mean|4420.248964089997|\n",
      "| stddev|7559.046597013129|\n",
      "|    min|             0.01|\n",
      "|    max|        296313.85|\n",
      "+-------+-----------------+\n",
      "\n",
      "+---------------+-----+\n",
      "|PromTimeMissing|count|\n",
      "+---------------+-----+\n",
      "|              1|  281|\n",
      "|              0|21941|\n",
      "+---------------+-----+\n",
      "\n",
      "+-------+-----------------+\n",
      "|summary|  PromTimeImputed|\n",
      "+-------+-----------------+\n",
      "|  count|            22222|\n",
      "|   mean|6.564605077252632|\n",
      "| stddev|4.627668213674665|\n",
      "|    min|              0.0|\n",
      "|    max|             39.0|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as f\n",
    "from pyspark.ml.feature import Imputer\n",
    "for variable in interval_with_missing:\n",
    "    # step 1. add a missing indicator: \n",
    "    indicator_name = variable+\"Missing\" # The variable name of the missing value indictor \n",
    "    data = data.withColumn(indicator_name, f.when(f.isnull(data[variable]),1).otherwise(0))\n",
    "    # do value count to verify.\n",
    "    data.groupBy(indicator_name).count().show()\n",
    "    # step 2: replace missing values with the mean\n",
    "    imputed_name = variable + \"Imputed\" # The variable name of the imputed variable\n",
    "    imputer = Imputer(strategy = 'mean', inputCols = [variable], outputCols = [imputed_name])\n",
    "    imputer_model = imputer.fit(data)\n",
    "    data = imputer_model.transform(data)\n",
    "    # run the describe to check whether the missing values has been imputed. \n",
    "    data.describe(imputed_name).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Missing value imputation for categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "|DemClusterGroup|count|\n",
      "+---------------+-----+\n",
      "|              F| 3949|\n",
      "|        unknown|  674|\n",
      "|              E| 2607|\n",
      "|              B| 4144|\n",
      "|              U|   54|\n",
      "|              D| 4378|\n",
      "|              C| 4566|\n",
      "|              A| 1850|\n",
      "+---------------+-----+\n",
      "\n",
      "+---------+-----+\n",
      "|DemGender|count|\n",
      "+---------+-----+\n",
      "|        F|12148|\n",
      "|        M| 5815|\n",
      "|        U| 4259|\n",
      "+---------+-----+\n",
      "\n",
      "+----------+-----+\n",
      "|    DemReg|count|\n",
      "+----------+-----+\n",
      "|  Scottish| 1368|\n",
      "|   unknown|  466|\n",
      "|South East| 8634|\n",
      "|South West|  691|\n",
      "|  Midlands| 6740|\n",
      "|     North| 4323|\n",
      "+----------+-----+\n",
      "\n",
      "+------------+-----+\n",
      "|    DemTVReg|count|\n",
      "+------------+-----+\n",
      "|      Ulster|  266|\n",
      "|      N East|  785|\n",
      "|      Border|  203|\n",
      "|     unknown|  465|\n",
      "|      S West|  691|\n",
      "|      London| 6189|\n",
      "|   Yorkshire| 1443|\n",
      "|        East| 1649|\n",
      "|      N Scot|  329|\n",
      "|      N West| 2096|\n",
      "|  C Scotland|  836|\n",
      "|    Midlands| 3122|\n",
      "|Wales & West| 1703|\n",
      "|  S & S East| 2445|\n",
      "+------------+-----+\n",
      "\n",
      "+---------+-----+\n",
      "|PromClass|count|\n",
      "+---------+-----+\n",
      "|      Tin| 6487|\n",
      "| Platinum|  840|\n",
      "|   Silver| 8572|\n",
      "|     Gold| 6323|\n",
      "+---------+-----+\n",
      "\n",
      "+---------+-----+\n",
      "|TargetBuy|count|\n",
      "+---------+-----+\n",
      "|        1| 5504|\n",
      "|        0|16718|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for variable in nominal_with_missing:\n",
    "    # if variable is DemGender, replace the missing values in the column with 'U'\n",
    "    if variable == 'DemGender':\n",
    "        # the method fillna can be used to replace missing values with a new value. \n",
    "        data = data.fillna({variable:'U'})\n",
    "    # for the other variable,  replace the missing values in the column with 'unknown'\n",
    "    else:\n",
    "        data = data.fillna({variable: \"unknown\"})\n",
    "    # run value_count to verify:\n",
    "    data.groupBy(variable).count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Categorical variable encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+----------------+-------------+---------------+----------------+\n",
      "|DemClusterGroupIndexed|DemGenderIndexed|DemRegIndexed|DemTVRegIndexed|PromClassIndexed|\n",
      "+----------------------+----------------+-------------+---------------+----------------+\n",
      "|                   0.0|             2.0|          1.0|            4.0|             2.0|\n",
      "|                   1.0|             2.0|          1.0|            4.0|             2.0|\n",
      "|                   1.0|             0.0|          1.0|            4.0|             0.0|\n",
      "|                   3.0|             1.0|          1.0|            1.0|             1.0|\n",
      "|                   5.0|             0.0|          1.0|            1.0|             1.0|\n",
      "|                   1.0|             2.0|          2.0|            3.0|             3.0|\n",
      "|                   5.0|             0.0|          1.0|            5.0|             1.0|\n",
      "|                   1.0|             1.0|          2.0|            8.0|             1.0|\n",
      "|                   3.0|             1.0|          1.0|            5.0|             0.0|\n",
      "|                   3.0|             0.0|          5.0|           10.0|             2.0|\n",
      "|                   3.0|             0.0|          2.0|            8.0|             2.0|\n",
      "|                   0.0|             1.0|          0.0|            0.0|             0.0|\n",
      "|                   3.0|             0.0|          1.0|            1.0|             0.0|\n",
      "|                   0.0|             0.0|          0.0|            2.0|             1.0|\n",
      "|                   0.0|             0.0|          0.0|            0.0|             1.0|\n",
      "|                   1.0|             1.0|          1.0|            1.0|             0.0|\n",
      "|                   2.0|             0.0|          1.0|            1.0|             2.0|\n",
      "|                   0.0|             0.0|          1.0|            5.0|             1.0|\n",
      "|                   4.0|             0.0|          3.0|            7.0|             1.0|\n",
      "|                   1.0|             2.0|          0.0|            0.0|             1.0|\n",
      "+----------------------+----------------+-------------+---------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "nominal_with_string = ['DemClusterGroup', 'DemGender', 'DemReg', 'DemTVReg', 'PromClass']\n",
    "for variable in nominal_with_string:\n",
    "    indexed_variable = variable+\"Indexed\" # The variable name of the indexed/encoded variable\n",
    "    indexer = StringIndexer(inputCol = variable, outputCol = indexed_variable)\n",
    "    indexer_model = indexer.fit(data)\n",
    "    data = indexer_model.transform(data)\n",
    "data.select('DemClusterGroupIndexed', 'DemGenderIndexed', 'DemRegIndexed', 'DemTVRegIndexed', 'PromClassIndexed').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.4. Dummy Coding:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+-------------------+----------------+------------------+-------------------+\n",
      "|DemClusterGroupIndexedVec|DemGenderIndexedVec|DemRegIndexedVec|DemTVRegIndexedVec|PromClassIndexedVec|\n",
      "+-------------------------+-------------------+----------------+------------------+-------------------+\n",
      "|            (7,[0],[1.0])|          (2,[],[])|   (5,[1],[1.0])|    (13,[4],[1.0])|      (3,[2],[1.0])|\n",
      "|            (7,[1],[1.0])|          (2,[],[])|   (5,[1],[1.0])|    (13,[4],[1.0])|      (3,[2],[1.0])|\n",
      "|            (7,[1],[1.0])|      (2,[0],[1.0])|   (5,[1],[1.0])|    (13,[4],[1.0])|      (3,[0],[1.0])|\n",
      "|            (7,[3],[1.0])|      (2,[1],[1.0])|   (5,[1],[1.0])|    (13,[1],[1.0])|      (3,[1],[1.0])|\n",
      "|            (7,[5],[1.0])|      (2,[0],[1.0])|   (5,[1],[1.0])|    (13,[1],[1.0])|      (3,[1],[1.0])|\n",
      "|            (7,[1],[1.0])|          (2,[],[])|   (5,[2],[1.0])|    (13,[3],[1.0])|          (3,[],[])|\n",
      "|            (7,[5],[1.0])|      (2,[0],[1.0])|   (5,[1],[1.0])|    (13,[5],[1.0])|      (3,[1],[1.0])|\n",
      "|            (7,[1],[1.0])|      (2,[1],[1.0])|   (5,[2],[1.0])|    (13,[8],[1.0])|      (3,[1],[1.0])|\n",
      "|            (7,[3],[1.0])|      (2,[1],[1.0])|   (5,[1],[1.0])|    (13,[5],[1.0])|      (3,[0],[1.0])|\n",
      "|            (7,[3],[1.0])|      (2,[0],[1.0])|       (5,[],[])|   (13,[10],[1.0])|      (3,[2],[1.0])|\n",
      "|            (7,[3],[1.0])|      (2,[0],[1.0])|   (5,[2],[1.0])|    (13,[8],[1.0])|      (3,[2],[1.0])|\n",
      "|            (7,[0],[1.0])|      (2,[1],[1.0])|   (5,[0],[1.0])|    (13,[0],[1.0])|      (3,[0],[1.0])|\n",
      "|            (7,[3],[1.0])|      (2,[0],[1.0])|   (5,[1],[1.0])|    (13,[1],[1.0])|      (3,[0],[1.0])|\n",
      "|            (7,[0],[1.0])|      (2,[0],[1.0])|   (5,[0],[1.0])|    (13,[2],[1.0])|      (3,[1],[1.0])|\n",
      "|            (7,[0],[1.0])|      (2,[0],[1.0])|   (5,[0],[1.0])|    (13,[0],[1.0])|      (3,[1],[1.0])|\n",
      "|            (7,[1],[1.0])|      (2,[1],[1.0])|   (5,[1],[1.0])|    (13,[1],[1.0])|      (3,[0],[1.0])|\n",
      "|            (7,[2],[1.0])|      (2,[0],[1.0])|   (5,[1],[1.0])|    (13,[1],[1.0])|      (3,[2],[1.0])|\n",
      "|            (7,[0],[1.0])|      (2,[0],[1.0])|   (5,[1],[1.0])|    (13,[5],[1.0])|      (3,[1],[1.0])|\n",
      "|            (7,[4],[1.0])|      (2,[0],[1.0])|   (5,[3],[1.0])|    (13,[7],[1.0])|      (3,[1],[1.0])|\n",
      "|            (7,[1],[1.0])|          (2,[],[])|   (5,[0],[1.0])|    (13,[0],[1.0])|      (3,[1],[1.0])|\n",
      "+-------------------------+-------------------+----------------+------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import OneHotEncoderEstimator\n",
    "# a list of nominal variables that needs to be dummy coded.\n",
    "indexed_nominal_variables = ['DemClusterGroupIndexed', 'DemGenderIndexed', 'DemRegIndexed', 'DemTVRegIndexed', 'PromClassIndexed']\n",
    "for variable in indexed_nominal_variables:\n",
    "    dummy_variable = variable+\"Vec\"\n",
    "    encoder = OneHotEncoderEstimator(inputCols=[variable],\n",
    "                                 outputCols=[dummy_variable])\n",
    "    model = encoder.fit(data)\n",
    "    data = model.transform(data)\n",
    "data.select('DemClusterGroupIndexedVec', 'DemGenderIndexedVec', 'DemRegIndexedVec', 'DemTVRegIndexedVec', 'PromClassIndexedVec').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DemAffl: double (nullable = true)\n",
      " |-- DemAge: double (nullable = true)\n",
      " |-- DemClusterGroup: string (nullable = false)\n",
      " |-- DemGender: string (nullable = false)\n",
      " |-- DemReg: string (nullable = false)\n",
      " |-- DemTVReg: string (nullable = false)\n",
      " |-- PromClass: string (nullable = false)\n",
      " |-- PromSpend: double (nullable = true)\n",
      " |-- PromTime: double (nullable = true)\n",
      " |-- TargetBuy: integer (nullable = true)\n",
      " |-- DemAfflMissing: integer (nullable = false)\n",
      " |-- DemAfflImputed: double (nullable = true)\n",
      " |-- DemAgeMissing: integer (nullable = false)\n",
      " |-- DemAgeImputed: double (nullable = true)\n",
      " |-- PromSpendMissing: integer (nullable = false)\n",
      " |-- PromSpendImputed: double (nullable = true)\n",
      " |-- PromTimeMissing: integer (nullable = false)\n",
      " |-- PromTimeImputed: double (nullable = true)\n",
      " |-- DemClusterGroupIndexed: double (nullable = false)\n",
      " |-- DemGenderIndexed: double (nullable = false)\n",
      " |-- DemRegIndexed: double (nullable = false)\n",
      " |-- DemTVRegIndexed: double (nullable = false)\n",
      " |-- PromClassIndexed: double (nullable = false)\n",
      " |-- DemClusterGroupIndexedVec: vector (nullable = true)\n",
      " |-- DemGenderIndexedVec: vector (nullable = true)\n",
      " |-- DemRegIndexedVec: vector (nullable = true)\n",
      " |-- DemTVRegIndexedVec: vector (nullable = true)\n",
      " |-- PromClassIndexedVec: vector (nullable = true)\n",
      "\n",
      "['DemAffl', 'DemAge', 'DemClusterGroup', 'DemGender', 'DemReg', 'DemTVReg', 'PromClass', 'PromSpend', 'PromTime', 'TargetBuy', 'DemAfflMissing', 'DemAfflImputed', 'DemAgeMissing', 'DemAgeImputed', 'PromSpendMissing', 'PromSpendImputed', 'PromTimeMissing', 'PromTimeImputed', 'DemClusterGroupIndexed', 'DemGenderIndexed', 'DemRegIndexed', 'DemTVRegIndexed', 'PromClassIndexed', 'DemClusterGroupIndexedVec', 'DemGenderIndexedVec', 'DemRegIndexedVec', 'DemTVRegIndexedVec', 'PromClassIndexedVec']\n"
     ]
    }
   ],
   "source": [
    "# now let's take a look at the schema of data again\n",
    "data.printSchema()\n",
    "# print all the variable in the dataframe\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.5 Construct the Features Vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let's create a list that includes all the independent variable we need to use in the machine learing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this independent variable list needs to include: \n",
    "# 1. For the nominal/categorical variable, only dummy variables created are included. \n",
    "# 2. For the interval variables\n",
    "#    a. Interval variables that do not have missing values are included.\n",
    "#    b. For the interval variables that have missing values,  the imputed variables and the missing value indicators are included.\n",
    "independent_variables = ['DemClusterGroupIndexedVec', 'DemGenderIndexedVec', 'DemRegIndexedVec', 'DemTVRegIndexedVec', 'PromClassIndexedVec', \"DemAfflMissing\", \"DemAfflImputed\", \"DemAgeMissing\", \"DemAgeImputed\", \"PromSpendMissing\", \"PromSpendImputed\", \"PromTimeMissing\", \"PromTimeImputed\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(DemAffl=10.0, DemAge=76.0, DemClusterGroup='C', DemGender='U', DemReg='Midlands', DemTVReg='Wales & West', PromClass='Gold', PromSpend=16000.0, PromTime=4.0, TargetBuy=0, DemAfflMissing=0, DemAfflImputed=10.0, DemAgeMissing=0, DemAgeImputed=76.0, PromSpendMissing=0, PromSpendImputed=16000.0, PromTimeMissing=0, PromTimeImputed=4.0, DemClusterGroupIndexed=0.0, DemGenderIndexed=2.0, DemRegIndexed=1.0, DemTVRegIndexed=4.0, PromClassIndexed=2.0, DemClusterGroupIndexedVec=SparseVector(7, {0: 1.0}), DemGenderIndexedVec=SparseVector(2, {}), DemRegIndexedVec=SparseVector(5, {1: 1.0}), DemTVRegIndexedVec=SparseVector(13, {4: 1.0}), PromClassIndexedVec=SparseVector(3, {2: 1.0}), features=SparseVector(38, {0: 1.0, 10: 1.0, 18: 1.0, 29: 1.0, 31: 10.0, 33: 76.0, 35: 16000.0, 37: 4.0})),\n",
       " Row(DemAffl=4.0, DemAge=49.0, DemClusterGroup='D', DemGender='U', DemReg='Midlands', DemTVReg='Wales & West', PromClass='Gold', PromSpend=6000.0, PromTime=5.0, TargetBuy=0, DemAfflMissing=0, DemAfflImputed=4.0, DemAgeMissing=0, DemAgeImputed=49.0, PromSpendMissing=0, PromSpendImputed=6000.0, PromTimeMissing=0, PromTimeImputed=5.0, DemClusterGroupIndexed=1.0, DemGenderIndexed=2.0, DemRegIndexed=1.0, DemTVRegIndexed=4.0, PromClassIndexed=2.0, DemClusterGroupIndexedVec=SparseVector(7, {1: 1.0}), DemGenderIndexedVec=SparseVector(2, {}), DemRegIndexedVec=SparseVector(5, {1: 1.0}), DemTVRegIndexedVec=SparseVector(13, {4: 1.0}), PromClassIndexedVec=SparseVector(3, {2: 1.0}), features=SparseVector(38, {1: 1.0, 10: 1.0, 18: 1.0, 29: 1.0, 31: 4.0, 33: 49.0, 35: 6000.0, 37: 5.0}))]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "assembler = VectorAssembler(inputCols=['DemClusterGroupIndexedVec', 'DemGenderIndexedVec', 'DemRegIndexedVec', 'DemTVRegIndexedVec', 'PromClassIndexedVec', 'DemAfflMissing', 'DemAfflImputed', \"DemAgeMissing\", 'DemAgeImputed', 'PromSpendMissing', 'PromSpendImputed', 'PromTimeMissing', 'PromTimeImputed'], outputCol='features')\n",
    "data = assembler.transform(data)\n",
    "data.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4. Training vs. test dataset partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = data.randomSplit([0.8, 0.2], seed=12345) # set seed to be 12345"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5. Classifcation: Training with Gradient-boosted tree classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TargetBuy as the dependent variable and its a classification. The algorithm used is Gradient-boosted tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use Gradient-booted tree.\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "algo = GBTClassifier(labelCol = 'TargetBuy', featuresCol = 'features',  maxIter = 10 )\n",
    "model = algo.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6. Make predictions using the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------------------+\n",
      "|TargetBuy|prediction|         probability|\n",
      "+---------+----------+--------------------+\n",
      "|        1|       0.0|[0.57371170405329...|\n",
      "|        0|       0.0|[0.57371170405329...|\n",
      "|        0|       0.0|[0.57371170405329...|\n",
      "|        0|       0.0|[0.84879546962180...|\n",
      "|        0|       0.0|[0.90568410810255...|\n",
      "|        0|       0.0|[0.90568410810255...|\n",
      "|        1|       0.0|[0.56675242663635...|\n",
      "|        1|       0.0|[0.57371170405329...|\n",
      "|        0|       0.0|[0.84879546962180...|\n",
      "|        0|       0.0|[0.83775176638211...|\n",
      "|        0|       0.0|[0.84879546962180...|\n",
      "|        0|       0.0|[0.57371170405329...|\n",
      "|        0|       0.0|[0.84879546962180...|\n",
      "|        0|       0.0|[0.89610967935479...|\n",
      "|        0|       0.0|[0.89610967935479...|\n",
      "|        1|       0.0|[0.57371170405329...|\n",
      "|        1|       0.0|[0.83775176638211...|\n",
      "|        1|       1.0|[0.26436986468342...|\n",
      "|        1|       1.0|[0.26436986468342...|\n",
      "|        1|       1.0|[0.25599025384377...|\n",
      "+---------+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# make prediction using test data\n",
    "predictions = model.transform(test)\n",
    "predictions.select(['TargetBuy','prediction', 'probability']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7. Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In MLlib the default metric used for evaluating classification models is area_under_roc (auc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.824653603874722"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# using spark ml to do model evaluation \n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator(labelCol='TargetBuy', metricName='areaUnderROC')\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.94      0.88      3394\n",
      "           1       0.69      0.41      0.52      1122\n",
      "\n",
      "   micro avg       0.81      0.81      0.81      4516\n",
      "   macro avg       0.76      0.68      0.70      4516\n",
      "weighted avg       0.79      0.81      0.79      4516\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_true = predictions.select(['TargetBuy']).collect()\n",
    "y_pred = predictions.select(['prediction']).collect()\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_true, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
